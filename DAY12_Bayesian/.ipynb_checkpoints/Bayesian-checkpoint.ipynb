{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯分类器\n",
    "**贝叶斯公式：**\n",
    "$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯决策论\n",
    "![](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体来说，若目标是最小化分类错误率，则误判损失$\\lambda_{ij}$可写为：\n",
    "![](./img/2.PNG)\n",
    "**前提小注：**判别式模型目标在于得到一个判别边界，而生成式模型目标在于数据与标签的联合分布。\n",
    "\n",
    "不难看出，欲使贝叶斯判定准则来最小化决策风险，首先要获得后验概率$P(c|x)$。然而，在现实任务中这通常难以获得。从这个角度来看，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$。 大体来说，主要有两种策略：给定X，可通过直接建模P(c|x)来预测c，这样得到的是”判别式模型“。也可以先对联合概率分布$P(c|x)$建模，然后再由此获得$P(c|x)$,这样得到的是”生成式模型“。显然决策树，神经网络，支持向量机都属于判别式范畴，而贝叶斯属于”生成式模型“。对于”生成式模型“来说，必然考虑：\n",
    "![](./img/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一句话总结：**使用贝叶斯分类的主旨是使用各种办法使得被分为正确的类的概率最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 极大似然估计\n",
    "* 估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。也就是将上面的$P(x|c)$假设其具有确定的形式且被$\\theta_c$唯一确定，则我们的任务就是利用训练集$D$来估计参数$\\theta_c$。估计参数有两个办法:\n",
    "\n",
    "    1. 认为参数虽然未知，但却是客观存在的固定值，因此可以通过**优化似然函数**等方法来求解。\n",
    "    1. 可假定参数服从一个先验分布，然后基于先验分布计算后验分布。\n",
    "\n",
    "这一节主要讲：极大似然估计法（MLE）\n",
    "\n",
    "* 一个例子：假如你去赌场，但是不知道能不能赚钱，你就在门口堵着\n",
    "出来一个人就问一个赚了还是赔了，如果问了5个人都说赚了，那么\n",
    "你就会认为，赚钱的概率肯定是非常大的。\n",
    "\n",
    "已知：（1）样本服从分布的模型， （2）观测到的样本\n",
    "\n",
    "求解：模型的参数\n",
    "\n",
    "**总的来说：**极大似然估计就是用来估计模型参数的统计学方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [朴素贝叶斯分类器](https://zhuanlan.zhihu.com/p/25493221)\n",
    "朴素贝叶斯算法仍然是流行的十大挖掘算法之一，该算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。接下来我们就详细介绍该算法的知识点及实际应用。\n",
    "\n",
    "* 采用属性条件独立性假设：假设每个属性独立的对分类结果发生影响。\n",
    "* 训练过程：基于训练集$D$来估计先验概率$P(c)$，并为每个属性估计条件概率$P(x_i|c)$,离散属性：按照频率计算，连续属性：假定其服从正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example：贝叶斯拼写检查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**求解：argmaxc P(c|w) -> argmaxc P(w|c) P(c) / P(w)**\n",
    "\n",
    "* P(c), 文章中出现一个正确拼写词 c 的概率, 也就是说, 在英语文章中, c 出现的概率有多大\n",
    "* P(w|c), 在用户想键入 c 的情况下敲成 w 的概率. 因为这个是代表用户会以多大的概率把 c 敲错成 w\n",
    "* argmaxc, 用来枚举所有可能的 c 并且选取概率最大的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把语料中的单词全部抽取出来, 转成小写, 并且去除单词中间的特殊符号\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "# 返回词频 \n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)#给没有出现但要返回的词默认lambda: 1\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    " \n",
    "NWORDS = train(words(open('./data/big.txt').read()))#字典格式{单词：单词频率}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要是遇到我们从来没有过见过的新词怎么办. 假如说一个词拼写完全正确, 但是语料库中没有包含这个词, 从而这个词也永远不会出现在训练集中. 于是, 我们就要返回出现这个词的概率是0. 这个情况不太妙, 因为概率为0这个代表了这个事件绝对不可能发生, 而在我们的概率模型中, 我们期望用一个很小的概率来代表这种情况. lambda: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**编辑距离:**\n",
    "\n",
    "两个词之间的编辑距离定义为使用了几次插入(在词中插入一个单字母), 删除(删除一个单字母), 交换(交换相邻两个字母), 替换(把一个字母换成另一个)的操作从一个词变到另一个词."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回所有与单词 w 编辑距离为 1 的集合\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set([word[0:i]+word[i+1:] for i in range(n)] +                     # 删除\n",
    "               [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] + # 交换\n",
    "               [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] + # 替换\n",
    "               [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])  # 插入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inq', 'tnk', 'fink', 'iwnk', 'snk', 'inl', 'mnk', 'eink', 'fnk', 'ynk', 'iqnk', 'isnk', 'inyk', 'ingk', 'nink', 'iny', 'inbk', 'inkf', 'inm', 'iunk', 'ank', 'infk', 'rnk', 'znk', 'pink', 'ignk', 'lnk', 'qnk', 'inkt', 'inhk', 'pnk', 'irk', 'aink', 'uink', 'inkb', 'inky', 'ilnk', 'inp', 'insk', 'imnk', 'gink', 'inj', 'ing', 'mink', 'gnk', 'ifnk', 'inkw', 'inkl', 'bink', 'idnk', 'iuk', 'bnk', 'inc', 'izk', 'ivnk', 'injk', 'qink', 'iik', 'iek', 'inkd', 'knk', 'inka', 'ivk', 'inx', 'intk', 'ihk', 'cnk', 'inek', 'zink', 'inak', 'kink', 'ifk', 'ijk', 'inkq', 'innk', 'inkc', 'ick', 'inzk', 'inkp', 'iynk', 'iink', 'hink', 'inkk', 'igk', 'ine', 'icnk', 'iwk', 'dink', 'iak', 'inkv', 'jink', 'inkm', 'ikn', 'cink', 'inrk', 'iznk', 'vnk', 'inqk', 'onk', 'inv', 'ini', 'idk', 'isk', 'inkn', 'indk', 'inpk', 'inko', 'iqk', 'inku', 'inke', 'inb', 'inr', 'inkj', 'inck', 'ind', 'xnk', 'dnk', 'itk', 'ixk', 'iyk', 'hnk', 'xink', 'inkh', 'inkx', 'inkg', 'inmk', 'nnk', 'enk', 'ipk', 'inki', 'tink', 'ixnk', 'link', 'unk', 'inks', 'inxk', 'inf', 'ink', 'itnk', 'ilk', 'ik', 'ikk', 'imk', 'inkz', 'sink', 'ins', 'yink', 'inkr', 'inz', 'ibk', 'inn', 'inw', 'rink', 'iok', 'inik', 'ino', 'nik', 'inuk', 'inwk', 'inu', 'inlk', 'inok', 'int', 'wnk', 'ipnk', 'iank', 'iknk', 'in', 'ienk', 'invk', 'nk', 'jnk', 'ihnk', 'oink', 'wink', 'irnk', 'ina', 'ibnk', 'inh', 'vink', 'ionk', 'ijnk'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('ink'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 something 编辑距离为2的单词居然达到了 114,324 个\n",
    "\n",
    "优化:在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词,只能返回 3 个单词: ‘smoothing’, ‘something’ 和 ‘soothing’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回所有与单词 w 编辑距离为 2 的集合\n",
    "#在这些编辑距离小于2的词中间, 只把那些正确的词作为候选词\n",
    "def edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def known(words): return set(w for w in words if w in NWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正常来说把一个元音拼成另一个的概率要大于辅音 (因为人常常把 hello 打成 hallo 这样); 把单词的第一个字母拼错的概率会相对小, 等等.但是为了简单起见, 选择了一个简单的方法: 编辑距离为1的正确单词比编辑距离为2的优先级高, 而编辑距离为0的正确单词优先级比编辑距离为1的高. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果known(set)非空, candidate 就会选取这个集合, 而不继续计算后面的\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('laste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example：sklearn朴素贝叶斯分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据  \n",
    "import numpy as np  \n",
    "#引入高斯朴素贝叶斯  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:贝叶斯新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据源：http://www.sogou.com/labs/resource/ca.php ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>theme</th>\n",
       "      <th>URL</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>时尚</td>\n",
       "      <td>常吃六类食物快速补充水分</td>\n",
       "      <td>http://lady.people.com.cn/GB/18248366.html</td>\n",
       "      <td>随着天气逐渐炎热，补水变得日益重要。据美国《跑步世界》杂志报道，喝水并不是为身体补充水分的唯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>时尚</td>\n",
       "      <td>情感：你是我的那盘菜　吃不起我走【２】</td>\n",
       "      <td>http://lady.people.com.cn/n/2012/0712/c1014-18...</td>\n",
       "      <td>我其实不想说这些话刺激他，他也是不得已。可是，我又该怎样说，怎样做？我只能走，离开这个伤心地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>时尚</td>\n",
       "      <td>揭秘不老女神刘晓庆的四任丈夫（图）</td>\n",
       "      <td>http://lady.people.com.cn/n/2012/0730/c1014-18...</td>\n",
       "      <td>５８岁刘晓庆最新嫩照Ｏ衷诘牧跸庆绝对看不出她已经５８岁了，她绝对可以秒杀刘亦菲、范冰冰这类美...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>时尚</td>\n",
       "      <td>样板潮爸　时尚圈里的父亲们</td>\n",
       "      <td>http://lady.people.com.cn/GB/18215232.html</td>\n",
       "      <td>导语：做了爸爸就是一种幸福，无论是领养还是亲生，更何况出现在影视剧中。时尚圈永远是需要领军人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>时尚</td>\n",
       "      <td>全球最美女人长啥样？中国最美女人酷似章子怡（图）</td>\n",
       "      <td>http://lady.people.com.cn/BIG5/n/2012/0727/c10...</td>\n",
       "      <td>全球最美女人合成图：：国整形外科教授李承哲，在国际学术杂志美容整形外科学会学报发表了考虑种族...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                     theme  \\\n",
       "4995       时尚              常吃六类食物快速补充水分   \n",
       "4996       时尚       情感：你是我的那盘菜　吃不起我走【２】   \n",
       "4997       时尚         揭秘不老女神刘晓庆的四任丈夫（图）   \n",
       "4998       时尚             样板潮爸　时尚圈里的父亲们   \n",
       "4999       时尚  全球最美女人长啥样？中国最美女人酷似章子怡（图）   \n",
       "\n",
       "                                                    URL  \\\n",
       "4995         http://lady.people.com.cn/GB/18248366.html   \n",
       "4996  http://lady.people.com.cn/n/2012/0712/c1014-18...   \n",
       "4997  http://lady.people.com.cn/n/2012/0730/c1014-18...   \n",
       "4998         http://lady.people.com.cn/GB/18215232.html   \n",
       "4999  http://lady.people.com.cn/BIG5/n/2012/0727/c10...   \n",
       "\n",
       "                                                content  \n",
       "4995  随着天气逐渐炎热，补水变得日益重要。据美国《跑步世界》杂志报道，喝水并不是为身体补充水分的唯...  \n",
       "4996  我其实不想说这些话刺激他，他也是不得已。可是，我又该怎样说，怎样做？我只能走，离开这个伤心地...  \n",
       "4997  ５８岁刘晓庆最新嫩照Ｏ衷诘牧跸庆绝对看不出她已经５８岁了，她绝对可以秒杀刘亦菲、范冰冰这类美...  \n",
       "4998  导语：做了爸爸就是一种幸福，无论是领养还是亲生，更何况出现在影视剧中。时尚圈永远是需要领军人...  \n",
       "4999  全球最美女人合成图：：国整形外科教授李承哲，在国际学术杂志美容整形外科学会学报发表了考虑种族...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = pd.read_table('./data/val.txt',names=['category','theme','URL','content'],encoding='utf-8')\n",
    "df_news = df_news.dropna()\n",
    "df_news.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  分词：使用结吧分词器 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿里巴巴集团昨日宣布，将在集团管理层面设立首席数据官岗位（Ｃｈｉｅｆ　Ｄａｔａ　Ｏｆｆｉｃｅｒ），阿里巴巴Ｂ２Ｂ公司ＣＥＯ陆兆禧将会出任上述职务，向集团ＣＥＯ马云直接汇报。＞菹ぃ和６月初的首席风险官职务任命相同，首席数据官亦为阿里巴巴集团在完成与雅虎股权谈判，推进“ｏｎｅ　ｃｏｍｐａｎｙ”目标后，在集团决策层面新增的管理岗位。０⒗锛团昨日表示，“变成一家真正意义上的数据公司”已是战略共识。记者刘夏 \n",
      " <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "content = df_news.content.values.tolist()\n",
    "print (content[1000],'\\n',type(content[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\adsss\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.865 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "content_S = []\n",
    "for line in content:\n",
    "    current_segment = jieba.lcut(line)\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n': #换行符\n",
    "        content_S.append(current_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['阿里巴巴',\n",
       " '集团',\n",
       " '昨日',\n",
       " '宣布',\n",
       " '，',\n",
       " '将',\n",
       " '在',\n",
       " '集团',\n",
       " '管理',\n",
       " '层面',\n",
       " '设立',\n",
       " '首席',\n",
       " '数据',\n",
       " '官',\n",
       " '岗位',\n",
       " '（',\n",
       " 'Ｃ',\n",
       " 'ｈ',\n",
       " 'ｉ',\n",
       " 'ｅ',\n",
       " 'ｆ',\n",
       " '\\u3000',\n",
       " 'Ｄ',\n",
       " 'ａ',\n",
       " 'ｔ',\n",
       " 'ａ',\n",
       " '\\u3000',\n",
       " 'Ｏ',\n",
       " 'ｆ',\n",
       " 'ｆ',\n",
       " 'ｉ',\n",
       " 'ｃ',\n",
       " 'ｅ',\n",
       " 'ｒ',\n",
       " '）',\n",
       " '，',\n",
       " '阿里巴巴',\n",
       " 'Ｂ',\n",
       " '２',\n",
       " 'Ｂ',\n",
       " '公司',\n",
       " 'Ｃ',\n",
       " 'Ｅ',\n",
       " 'Ｏ',\n",
       " '陆兆禧',\n",
       " '将',\n",
       " '会',\n",
       " '出任',\n",
       " '上述',\n",
       " '职务',\n",
       " '，',\n",
       " '向',\n",
       " '集团',\n",
       " 'Ｃ',\n",
       " 'Ｅ',\n",
       " 'Ｏ',\n",
       " '马云',\n",
       " '直接',\n",
       " '汇报',\n",
       " '。',\n",
       " '＞',\n",
       " '菹',\n",
       " 'ぃ',\n",
       " '和',\n",
       " '６',\n",
       " '月初',\n",
       " '的',\n",
       " '首席',\n",
       " '风险',\n",
       " '官',\n",
       " '职务',\n",
       " '任命',\n",
       " '相同',\n",
       " '，',\n",
       " '首席',\n",
       " '数据',\n",
       " '官亦为',\n",
       " '阿里巴巴',\n",
       " '集团',\n",
       " '在',\n",
       " '完成',\n",
       " '与',\n",
       " '雅虎',\n",
       " '股权',\n",
       " '谈判',\n",
       " '，',\n",
       " '推进',\n",
       " '“',\n",
       " 'ｏ',\n",
       " 'ｎ',\n",
       " 'ｅ',\n",
       " '\\u3000',\n",
       " 'ｃ',\n",
       " 'ｏ',\n",
       " 'ｍ',\n",
       " 'ｐ',\n",
       " 'ａ',\n",
       " 'ｎ',\n",
       " 'ｙ',\n",
       " '”',\n",
       " '目标',\n",
       " '后',\n",
       " '，',\n",
       " '在',\n",
       " '集团',\n",
       " '决策',\n",
       " '层面',\n",
       " '新增',\n",
       " '的',\n",
       " '管理',\n",
       " '岗位',\n",
       " '。',\n",
       " '０',\n",
       " '⒗',\n",
       " '锛',\n",
       " '团',\n",
       " '昨日',\n",
       " '表示',\n",
       " '，',\n",
       " '“',\n",
       " '变成',\n",
       " '一家',\n",
       " '真正',\n",
       " '意义',\n",
       " '上',\n",
       " '的',\n",
       " '数据',\n",
       " '公司',\n",
       " '”',\n",
       " '已',\n",
       " '是',\n",
       " '战略',\n",
       " '共识',\n",
       " '。',\n",
       " '记者',\n",
       " '刘夏']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_S[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           content_S\n",
       "0  [经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...\n",
       "1  [呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...\n",
       "2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...\n",
       "3  [清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...\n",
       "4  [在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content=pd.DataFrame({'content_S':content_S})#转为pandas数据格式\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        !\n",
       "1        \"\n",
       "2        #\n",
       "3        $\n",
       "4        %"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./data/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除停用词\n",
    "def drop_stopwords(contents,stopwords):\n",
    "    contents_clean = []\n",
    "    all_words = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "            all_words.append(str(word))\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "contents = df_content.content_S.values.tolist() #将dataframe格式转为列表格式   \n",
    "stopwords = stopwords.stopword.values.tolist() #将dataframe格式转为列表格式\n",
    "contents_clean,all_words = drop_stopwords(contents,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      contents_clean\n",
       "0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...\n",
       "1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...\n",
       "2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...\n",
       "3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...\n",
       "4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content=pd.DataFrame({'contents_clean':contents_clean})\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>经销商</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>电话</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>试驾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>订车</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ｕ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_words\n",
       "0       经销商\n",
       "1        电话\n",
       "2        试驾\n",
       "3        订车\n",
       "4         Ｕ"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words=pd.DataFrame({'all_words':all_words})\n",
    "df_all_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>中</td>\n",
       "      <td>5199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>中国</td>\n",
       "      <td>3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88255</th>\n",
       "      <td>说</td>\n",
       "      <td>3055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104747</th>\n",
       "      <td>Ｓ</td>\n",
       "      <td>2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>万</td>\n",
       "      <td>2390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       all_words  count\n",
       "4077           中   5199\n",
       "4209          中国   3115\n",
       "88255          说   3055\n",
       "104747         Ｓ   2646\n",
       "1373           万   2390"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count=df_all_words.groupby(by=['all_words'])['all_words'].agg({\"count\"})\n",
    "words_count=words_count.reset_index().sort_values(by=[\"count\"],ascending=False)\n",
    "words_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [TF-IDF ](https://blog.csdn.net/zrc199021/article/details/53728499)：提取关键词###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。”\n",
      "------------------------------------------------------------------------------------\n",
      "耐克  阿迪达斯  欧洲杯  球衣  西班牙\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "index = 2400\n",
    "print (df_news['content'][index])\n",
    "print('------------------------------------------------------------------------------------')\n",
    "content_S_str = \"\".join(content_S[index])  \n",
    "print (\"  \".join(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LDA ](https://blog.csdn.net/aws3217150/article/details/53840029)：主题模型###\n",
    "\n",
    "格式要求：list of list形式，分词好的的整个语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "#http://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做映射，相当于词袋\n",
    "dictionary = corpora.Dictionary(contents_clean)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in contents_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) #类似Kmeans自己指定K值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005*\"剧\" + 0.004*\"剧中\" + 0.004*\"中\" + 0.003*\"Ｓ\" + 0.003*\"Ｄ\"\n"
     ]
    }
   ],
   "source": [
    "#一号分类结果\n",
    "print (lda.print_topic(1, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006*\"中\" + 0.005*\"节目\" + 0.004*\"粉丝\" + 0.004*\"音乐\" + 0.003*\"歌曲\"\n",
      "0.005*\"剧\" + 0.004*\"剧中\" + 0.004*\"中\" + 0.003*\"Ｓ\" + 0.003*\"Ｄ\"\n",
      "0.009*\"中\" + 0.006*\"说\" + 0.004*\"男人\" + 0.004*\"孩子\" + 0.003*\"做\"\n",
      "0.007*\"卫视\" + 0.006*\"中\" + 0.005*\"中国\" + 0.003*\"评委\" + 0.003*\"工作\"\n",
      "0.011*\"观众\" + 0.006*\"ｅ\" + 0.006*\"中国\" + 0.005*\"中\" + 0.005*\"ｏ\"\n",
      "0.014*\"万\" + 0.008*\"号\" + 0.007*\"转\" + 0.004*\"Ｌ\" + 0.004*\"ｅ\"\n",
      "0.006*\"饰演\" + 0.004*\"万\" + 0.004*\"播出\" + 0.003*\"中\" + 0.003*\"恋情\"\n",
      "0.010*\"电影\" + 0.006*\"中\" + 0.004*\"说\" + 0.003*\"表演\" + 0.003*\"导演\"\n",
      "0.004*\"中\" + 0.004*\"说\" + 0.003*\"中国\" + 0.003*\"学校\" + 0.002*\"发展\"\n",
      "0.024*\"ａ\" + 0.020*\"ｅ\" + 0.016*\"ｉ\" + 0.015*\"ｏ\" + 0.015*\"ｎ\"\n",
      "0.011*\"男人\" + 0.010*\"女人\" + 0.006*\"说\" + 0.003*\"中\" + 0.003*\"比赛\"\n",
      "0.006*\"中\" + 0.005*\"文化\" + 0.004*\"决赛\" + 0.004*\"陈坤\" + 0.003*\"电影\"\n",
      "0.005*\"选手\" + 0.005*\"中\" + 0.004*\"比赛\" + 0.004*\"Ｔ\" + 0.004*\"Ｍ\"\n",
      "0.006*\"中\" + 0.003*\"球队\" + 0.003*\"说\" + 0.003*\"中国\" + 0.002*\"作品\"\n",
      "0.005*\"节目\" + 0.004*\"中国\" + 0.004*\"中\" + 0.004*\"Ｓ\" + 0.004*\"Ｍ\"\n",
      "0.005*\"中\" + 0.004*\"饰演\" + 0.002*\"性感\" + 0.002*\"部队\" + 0.002*\"工作\"\n",
      "0.007*\"肌肤\" + 0.005*\"吃\" + 0.005*\"皮肤\" + 0.005*\"中\" + 0.004*\"含有\"\n",
      "0.006*\"比赛\" + 0.006*\"中\" + 0.004*\"职场\" + 0.003*\"饰演\" + 0.002*\"专辑\"\n",
      "0.005*\"天籁\" + 0.004*\"公司\" + 0.003*\"中\" + 0.003*\"Ｉ\" + 0.003*\"产品\"\n",
      "0.006*\"中国\" + 0.006*\"考生\" + 0.003*\"中\" + 0.003*\"训练\" + 0.003*\"文化\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=20, num_words=5):\n",
    "    print (topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         contents_clean label\n",
       "4995  [天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...    时尚\n",
       "4996  [不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...    时尚\n",
       "4997  [岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...    时尚\n",
       "4998  [导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...    时尚\n",
       "4999  [全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...    时尚"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame({'contents_clean':contents_clean,'label':df_news['category']})\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['汽车', '财经', '科技', '健康', '体育', '教育', '文化', '军事', '娱乐', '时尚'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.unique()#标签的总共类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      contents_clean  label\n",
       "0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...      1\n",
       "1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...      1\n",
       "2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...      1\n",
       "3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...      1\n",
       "4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...      1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping = {\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0}\n",
    "df_train['label'] = df_train['label'].map(label_mapping)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用贝叶斯进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'上海'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = x_train.flatten()\n",
    "x_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except:\n",
    "        print (line_index,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750\n"
     ]
    }
   ],
   "source": [
    "print (len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一个简单的小示例：将文章转换为单词-文档矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]]\n",
      "[2 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 为了保留一些局部顺序信息，我们可以在抽取词的1-grams（词本身）之外，再抽取2-grams："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'cat cat', 'cat fish', 'dog', 'dog cat', 'fish', 'fish bird']\n",
      "[[0 1 0 1 1 1 1 0]\n",
      " [0 2 1 0 1 1 0 0]\n",
      " [1 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 0 0 0]]\n",
      "[2 3 1 1 2 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "\n",
    "\n",
    "print(cv_fit.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)#抽取词频最高的4000个作为特征\n",
    "vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "         print (line_index,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)#抽取TFIDF值最高的4000词作为特征\n",
    "vectorizer.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(vectorizer.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8152"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vectorizer.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM算法\n",
    "* EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。它的算法思想很简单，基本来说由两步组成\n",
    "\n",
    "    1. E步：求期望（Expectation）\n",
    "    1. M步：求极大（Maximization）\n",
    "[EM算法详解](https://zhuanlan.zhihu.com/p/26162237)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
